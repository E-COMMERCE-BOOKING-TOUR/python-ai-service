import torch
import subprocess
import os
from pathlib import Path
from peft import PeftModel
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)
from peft import LoraConfig, get_peft_model

# ===============================
# 2. CONFIG
# ===============================
MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"

# Use absolute paths based on script location
BASE_DIR = Path(__file__).parent.parent / "model"
TRAIN_FILE = str(Path(__file__).parent / "dataset" / "training.jsonl")
TEST_FILE = str(Path(__file__).parent / "dataset" / "test.jsonl")
OUTPUT_DIR = str(BASE_DIR / "lora-qwen2.5-1.5b-final")
MERGED_DIR = str(BASE_DIR / "qwen_merged")
GGUF_FILE = str(BASE_DIR / "qwen2.5-1.5b-tour-assistant-q4.gguf")
TEMP_GGUF = str(BASE_DIR / "temp.gguf")

# Path to llama.cpp (adjust if needed)
LLAMA_CPP_DIR = Path(__file__).parent / "llama.cpp"

MAX_LENGTH = 512
BATCH_SIZE = 2  # TƒÉng nh·∫π n·∫øu VRAM cho ph√©p
GRAD_ACCUM = 4
EPOCHS = 3
LR = 2e-4

# ===============================
# 3. TOKENIZER
# ===============================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"  # Trainer chu·∫©n th√≠ch padding ph·∫£i

# ===============================
# 4. LOAD MODEL (4BIT)
# ===============================
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False

# ===============================
# 5. LORA CONFIG (T·ªêI ∆ØU CHO QWEN)
# ===============================
lora_config = LoraConfig(
    r=16,  # TƒÉng r l√™n ch√∫t ƒë·ªÉ h·ªçc t·ªët h∆°n
    lora_alpha=32,
    # Th√™m c√°c module gate_proj, up_proj, down_proj gi√∫p Qwen th√¥ng minh h∆°n nhi·ªÅu
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ===============================
# 6. DATASET & PREPROCESS (QUAN TR·ªåNG)
# ===============================
dataset = load_dataset("json", data_files={"train": TRAIN_FILE, "test": TEST_FILE})


def preprocess(example):
    # 1. T·∫°o Message chu·∫©n ChatML
    messages = [
        {
            "role": "system",
            "content": "B·∫°n l√† tr·ª£ l√Ω h·ªó tr·ª£ kh√°ch h√†ng. H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c.",
        },
        {"role": "user", "content": example["input"]},
        {"role": "assistant", "content": example["output"]},
    ]

    # 2. Format th√†nh text c√≥ ch·ª©a <|im_start|>, <|im_end|>...
    # Quan tr·ªçng: add_generation_prompt=False ƒë·ªÉ n√≥ t·ª± th√™m EOS v√†o cu·ªëi
    full_text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=False
    )

    # 3. Tokenize to√†n b·ªô chu·ªói
    tokenized = tokenizer(
        full_text,
        truncation=True,
        max_length=MAX_LENGTH,
        padding=False,  # ƒê·ªÉ DataCollator x·ª≠ l√Ω padding ƒë·ªông s·∫Ω ti·∫øt ki·ªám b·ªô nh·ªõ h∆°n
        add_special_tokens=False,
    )

    # 4. T·∫°o Masking (Ch·ªâ t√≠nh loss cho ph·∫ßn Assistant tr·∫£ l·ªùi)
    # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi c·ªßa Assistant
    # Trong ChatML, ph·∫ßn tr·∫£ l·ªùi b·∫Øt ƒë·∫ßu sau header: "<|im_start|>assistant\n"
    input_ids = tokenized["input_ids"]
    labels = input_ids.copy()

    # T√≠nh to√°n prompt (System + User) ƒë·ªÉ g√°n -100
    # M·∫πo: T·∫°o prompt gi·∫£ ƒë·ªÉ ƒëo ƒë·ªô d√†i
    prompt_messages = messages[:-1]  # B·ªè ph·∫ßn assistant
    prompt_text = tokenizer.apply_chat_template(
        prompt_messages, tokenize=False, add_generation_prompt=True
    )
    prompt_ids = tokenizer(
        prompt_text, truncation=True, max_length=MAX_LENGTH, add_special_tokens=False
    )["input_ids"]
    prompt_len = len(prompt_ids)

    # G√°n -100 cho ph·∫ßn Prompt
    for i in range(len(labels)):
        if i < prompt_len:
            labels[i] = -100
        else:
            labels[i] = input_ids[i]  # Gi·ªØ nguy√™n ph·∫ßn output ƒë·ªÉ t√≠nh loss

    tokenized["labels"] = labels
    return tokenized


tokenized_dataset = dataset.map(
    preprocess, remove_columns=dataset["train"].column_names
)

# ===============================
# 7. TRAINER SETUP
# ===============================
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    learning_rate=LR,
    num_train_epochs=EPOCHS,
    logging_steps=10,
    fp16=True,
    save_strategy="epoch",
    eval_strategy="epoch",
    report_to="none",
)

# D√πng DataCollatorForSeq2Seq ƒë·ªÉ padding ƒë·ªông (dynamic padding) -> Ti·∫øt ki·ªám VRAM v√† nhanh h∆°n
data_collator = DataCollatorForSeq2Seq(
    tokenizer, pad_to_multiple_of=8, return_tensors="pt", padding=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    data_collator=data_collator,
)

# ===============================
# 8. TRAIN & SAVE
# ===============================
print("ƒêang b·∫Øt ƒë·∫ßu train...")
trainer.train()

print("ƒêang l∆∞u LoRA adapter...")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# ===============================
# 9. CLEANUP MEMORY BEFORE MERGE
# ===============================
print("ƒêang d·ªçn d·∫πp b·ªô nh·ªõ tr∆∞·ªõc khi merge...")
del model
del trainer
torch.cuda.empty_cache()

# ===============================
# 10. MERGE LORA INTO BASE MODEL
# ===============================
print("ƒêang g·ªôp LoRA v√†o base model...")
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="cpu",  # Merge tr√™n CPU ƒë·ªÉ ti·∫øt ki·ªám VRAM
    trust_remote_code=True,
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)
merged_model = merged_model.merge_and_unload()  # G·ªôp h·∫≥n v√†o

# Create output directory if not exists
os.makedirs(MERGED_DIR, exist_ok=True)

merged_model.save_pretrained(MERGED_DIR)
tokenizer.save_pretrained(MERGED_DIR)
print(f"ƒê√£ g·ªôp xong model t·∫°i: {MERGED_DIR}")

# Cleanup merged model from memory
del base_model
del merged_model
torch.cuda.empty_cache()

# ===============================
# 11. CONVERT TO GGUF FORMAT
# ===============================
print("ƒêang chuy·ªÉn ƒë·ªïi sang GGUF...")

convert_script = LLAMA_CPP_DIR / "convert_hf_to_gguf.py"
quantize_bin = LLAMA_CPP_DIR / "llama-quantize"

# Check if llama.cpp tools exist
if not convert_script.exists():
    print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y {convert_script}")
    print("H√£y clone llama.cpp v√† build tr∆∞·ªõc:")
    print("  git clone https://github.com/ggerganov/llama.cpp")
    print("  cd llama.cpp && make")
else:
    try:
        # Step 1: Convert HF to GGUF (FP16)
        subprocess.run(
            [
                "python",
                str(convert_script),
                MERGED_DIR,
                "--outfile",
                TEMP_GGUF,
                "--outtype",
                "f16",
            ],
            check=True,
        )
        print(f"ƒê√£ convert sang GGUF FP16: {TEMP_GGUF}")

        # Step 2: Quantize to 4-bit (Q4_K_M - t·ªët cho inference)
        if quantize_bin.exists():
            subprocess.run(
                [
                    str(quantize_bin),
                    TEMP_GGUF,
                    GGUF_FILE,
                    "q4_k_m",
                ],
                check=True,
            )
            print(f"ƒê√£ n√©n xu·ªëng 4-bit: {GGUF_FILE}")

            # Clean up temp file
            if os.path.exists(TEMP_GGUF):
                os.remove(TEMP_GGUF)
        else:
            print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y {quantize_bin}")
            print("File GGUF FP16 ƒë√£ c√≥ t·∫°i:", TEMP_GGUF)
            print("H√£y ch·∫°y quantize th·ªß c√¥ng:")
            print(f"  ./llama.cpp/llama-quantize {TEMP_GGUF} {GGUF_FILE} q4_k_m")

    except subprocess.CalledProcessError as e:
        print(f"‚ùå L·ªói khi convert GGUF: {e}")
        print("H√£y ki·ªÉm tra llama.cpp ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t ƒë√∫ng ch∆∞a.")

print("=" * 50)
print("üéâ HO√ÄN T·∫§T!")
print(f"   LoRA adapter: {OUTPUT_DIR}")
print(f"   Merged model: {MERGED_DIR}")
if os.path.exists(GGUF_FILE):
    print(f"   GGUF file:    {GGUF_FILE}")
print("=" * 50)
