{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Fine-tune Qwen 2.5 with LoRA for Tour Assistant\n",
                "\n",
                "This notebook will guide you through:\n",
                "1. Install dependencies\n",
                "2. Upload dataset\n",
                "3. Fine-tune model with LoRA\n",
                "4. Merge and convert to GGUF\n",
                "\n",
                "**Requirements:** GPU T4 or higher (Colab Free is sufficient)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Install Dependencies"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q torch transformers datasets peft bitsandbytes accelerate sentencepiece\n",
                "!pip install -q huggingface_hub"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Configuration & Import"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import os\n",
                "from peft import PeftModel, LoraConfig, get_peft_model\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForCausalLM,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorForSeq2Seq,\n",
                ")\n",
                "\n",
                "# Check GPU\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ===============================\n",
                "# CONFIG - MODIFY HERE\n",
                "# ===============================\n",
                "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
                "\n",
                "# Paths (in Colab)\n",
                "TRAIN_FILE = \"/content/dataset/training.jsonl\"\n",
                "TEST_FILE = \"/content/dataset/test.jsonl\"\n",
                "OUTPUT_DIR = \"/content/lora-adapter\"\n",
                "MERGED_DIR = \"/content/qwen_merged\"\n",
                "GGUF_FILE = \"/content/qwen2.5-1.5b-tour-assistant-q4.gguf\"\n",
                "\n",
                "# Training hyperparameters\n",
                "MAX_LENGTH = 512\n",
                "BATCH_SIZE = 2      # Increase to 4 if you have a better GPU\n",
                "GRAD_ACCUM = 4\n",
                "EPOCHS = 3\n",
                "LR = 2e-4"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Upload Dataset\n",
                "\n",
                "Create folder and upload `training.jsonl` and `test.jsonl`\n",
                "\n",
                "Format for each line in JSONL file:\n",
                "```json\n",
                "{\"input\": \"User question\", \"output\": \"Assistant response\"}\n",
                "```"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Create dataset folder\n",
                "!mkdir -p /content/dataset\n",
                "\n",
                "# Upload files manually or mount Google Drive\n",
                "from google.colab import files\n",
                "print(\"Upload training.jsonl:\")\n",
                "uploaded = files.upload()\n",
                "for fn in uploaded.keys():\n",
                "    !mv \"{fn}\" /content/dataset/\n",
                "\n",
                "print(\"\\nUpload test.jsonl:\")\n",
                "uploaded = files.upload()\n",
                "for fn in uploaded.keys():\n",
                "    !mv \"{fn}\" /content/dataset/"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# OR: Mount Google Drive if dataset already exists there\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "# TRAIN_FILE = \"/content/drive/MyDrive/dataset/training.jsonl\"\n",
                "# TEST_FILE = \"/content/drive/MyDrive/dataset/test.jsonl\""
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Verify dataset\n",
                "!echo \"=== Training samples ===\"\n",
                "!head -2 {TRAIN_FILE}\n",
                "!echo \"\\n=== Test samples ===\"\n",
                "!head -2 {TEST_FILE}\n",
                "!echo \"\\n=== Counts ===\"\n",
                "!wc -l {TRAIN_FILE} {TEST_FILE}"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Load Tokenizer & Model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "print(f\"Vocab size: {tokenizer.vocab_size}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Load model with 4-bit quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "model.config.use_cache = False\n",
                "\n",
                "print(f\"Model loaded on: {model.device}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. LoRA Configuration"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
                "    ],\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "model.print_trainable_parameters()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Prepare Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "dataset = load_dataset(\"json\", data_files={\"train\": TRAIN_FILE, \"test\": TEST_FILE})\n",
                "print(dataset)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "def preprocess(example):\n",
                "    \"\"\"Tokenize and create labels with prompt masking\"\"\"\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": \"Ban la tro ly ho tro khach hang. Hay tra loi ngan gon va chinh xac.\"},\n",
                "        {\"role\": \"user\", \"content\": example[\"input\"]},\n",
                "        {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
                "    ]\n",
                "\n",
                "    # Format ChatML\n",
                "    full_text = tokenizer.apply_chat_template(\n",
                "        messages, tokenize=False, add_generation_prompt=False\n",
                "    )\n",
                "\n",
                "    # Tokenize\n",
                "    tokenized = tokenizer(\n",
                "        full_text,\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding=False,\n",
                "        add_special_tokens=False,\n",
                "    )\n",
                "\n",
                "    # Create labels with masking\n",
                "    input_ids = tokenized[\"input_ids\"]\n",
                "    labels = input_ids.copy()\n",
                "\n",
                "    # Calculate prompt length (system + user)\n",
                "    prompt_messages = messages[:-1]\n",
                "    prompt_text = tokenizer.apply_chat_template(\n",
                "        prompt_messages, tokenize=False, add_generation_prompt=True\n",
                "    )\n",
                "    prompt_ids = tokenizer(\n",
                "        prompt_text, truncation=True, max_length=MAX_LENGTH, add_special_tokens=False\n",
                "    )[\"input_ids\"]\n",
                "    prompt_len = len(prompt_ids)\n",
                "\n",
                "    # Mask prompt with -100 (ignore in loss calculation)\n",
                "    for i in range(len(labels)):\n",
                "        if i < prompt_len:\n",
                "            labels[i] = -100\n",
                "\n",
                "    tokenized[\"labels\"] = labels\n",
                "    return tokenized\n",
                "\n",
                "tokenized_dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n",
                "print(f\"Train: {len(tokenized_dataset['train'])} samples\")\n",
                "print(f\"Test: {len(tokenized_dataset['test'])} samples\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7. Training"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM,\n",
                "    learning_rate=LR,\n",
                "    num_train_epochs=EPOCHS,\n",
                "    logging_steps=10,\n",
                "    fp16=True,\n",
                "    save_strategy=\"epoch\",\n",
                "    eval_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    report_to=\"none\",\n",
                ")\n",
                "\n",
                "data_collator = DataCollatorForSeq2Seq(\n",
                "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset[\"train\"],\n",
                "    eval_dataset=tokenized_dataset[\"test\"],\n",
                "    data_collator=data_collator,\n",
                ")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# START TRAINING\n",
                "print(\"=\"*50)\n",
                "print(\"Starting training...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "trainer.train()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Save LoRA adapter\n",
                "print(\"Saving LoRA adapter...\")\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(f\"Saved at: {OUTPUT_DIR}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 8. Test Model After Training"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Test the model\n",
                "def generate_response(prompt):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": \"Ban la tro ly ho tro khach hang. Hay tra loi ngan gon va chinh xac.\"},\n",
                "        {\"role\": \"user\", \"content\": prompt},\n",
                "    ]\n",
                "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=256,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "        )\n",
                "\n",
                "    generated_ids = outputs[0][len(inputs.input_ids[0]):]\n",
                "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
                "\n",
                "# Test\n",
                "test_questions = [\n",
                "    \"Tour Da Lat co gia bao nhieu?\",\n",
                "    \"Toi muon huy dat tour\",\n",
                "    \"Co tour nao di vao cuoi tuan khong?\",\n",
                "]\n",
                "\n",
                "for q in test_questions:\n",
                "    print(f\"\\nUser: {q}\")\n",
                "    print(f\"Bot: {generate_response(q)}\")\n",
                "    print(\"-\"*50)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 9. Merge LoRA into Base Model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Cleanup memory\n",
                "del model\n",
                "del trainer\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "print(\"VRAM released\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Load base model (FP16, on CPU to save VRAM)\n",
                "print(\"Loading base model...\")\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"cpu\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Merge LoRA\n",
                "print(\"Merging LoRA adapter...\")\n",
                "merged_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
                "merged_model = merged_model.merge_and_unload()\n",
                "\n",
                "# Save\n",
                "print(f\"Saving merged model to {MERGED_DIR}...\")\n",
                "os.makedirs(MERGED_DIR, exist_ok=True)\n",
                "merged_model.save_pretrained(MERGED_DIR)\n",
                "tokenizer.save_pretrained(MERGED_DIR)\n",
                "\n",
                "print(f\"Merged model saved at: {MERGED_DIR}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cleanup\n",
                "del base_model\n",
                "del merged_model\n",
                "torch.cuda.empty_cache()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 10. Convert to GGUF"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Clone and build llama.cpp\n",
                "!git clone https://github.com/ggerganov/llama.cpp /content/llama.cpp\n",
                "!cd /content/llama.cpp && make -j4"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Install dependencies for convert script\n",
                "!pip install -q gguf"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Convert HF -> GGUF (FP16)\n",
                "!python /content/llama.cpp/convert_hf_to_gguf.py {MERGED_DIR} \\\n",
                "    --outfile /content/temp.gguf \\\n",
                "    --outtype f16\n",
                "\n",
                "print(\"Converted to GGUF FP16\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Quantize to 4-bit (Q4_K_M - balance between quality and size)\n",
                "!/content/llama.cpp/llama-quantize /content/temp.gguf {GGUF_FILE} q4_k_m\n",
                "\n",
                "# Remove temp file\n",
                "!rm /content/temp.gguf\n",
                "\n",
                "print(f\"Quantized to 4-bit: {GGUF_FILE}\")\n",
                "!ls -lh {GGUF_FILE}"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 11. Download GGUF File"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Option 1: Download directly\n",
                "from google.colab import files\n",
                "files.download(GGUF_FILE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Option 2: Copy to Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "!cp {GGUF_FILE} /content/drive/MyDrive/\n",
                "print(\"Copied to Google Drive!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Done!\n",
                "\n",
                "You now have the GGUF file to use with `llama-cpp-python`.\n",
                "\n",
                "### Usage in Python:\n",
                "```python\n",
                "from llama_cpp import Llama\n",
                "\n",
                "llm = Llama(model_path=\"qwen2.5-1.5b-tour-assistant-q4.gguf\", n_ctx=2048)\n",
                "\n",
                "output = llm(\n",
                "    \"<|im_start|>user\\nTour Da Lat gia bao nhieu?<|im_end|>\\n<|im_start|>assistant\\n\",\n",
                "    max_tokens=256,\n",
                "    stop=[\"<|im_end|>\"],\n",
                ")\n",
                "print(output[\"choices\"][0][\"text\"])\n",
                "```"
            ],
            "metadata": {}
        }
    ]
}